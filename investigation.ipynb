{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"D:\\GitHub\\Incursion-Damage-Mods\\webcrawler\\HeatSinkOutput.csv\")\n",
    "df2 = pd.read_csv(\"D:\\GitHub\\Incursion-Damage-Mods\\webcrawler\\MagStabOutput.csv\")\n",
    "df3 = pd.read_csv(\"D:\\GitHub\\Incursion-Damage-Mods\\webcrawler\\GyroStabOutput.csv\")\n",
    "\n",
    "df_merge = pd.concat([df1, df2], ignore_index=True)\n",
    "df_merge = pd.concat([df_merge, df3], ignore_index=True)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://esi.evetech.net/latest/universe/regions/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "region_ids = response.json()\n",
    "print(f\"Regions to scan: {len(region_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = []\n",
    "for region_id in region_ids:\n",
    "    total_pages = 1\n",
    "    current_page = 1\n",
    "    base_url = \"https://esi.evetech.net/latest/contracts/public/{}/\".format(region_id)\n",
    "    params = {\"datasource\": \"tranquility\", \"page\": current_page}\n",
    "    \n",
    "    while params[\"page\"] <= total_pages:\n",
    "        print(base_url)\n",
    "        print(params)\n",
    "        response = requests.get(base_url, params=params)\n",
    "        print(response.headers)\n",
    "        print(response)\n",
    "        if total_pages == 1:\n",
    "            total_pages = int(response.headers.get(\"X-Pages\"))\n",
    "            \n",
    "        if response.status_code == 200:\n",
    "            raw_contracts = response.json()\n",
    "            item_exchange_contracts = [contract for contract in raw_contracts if contract[\"type\"] == \"item_exchange\"]\n",
    "            contracts.extend(item_exchange_contracts)\n",
    "            params[\"page\"] += 1\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected API Response\")\n",
    "        if \"ETag\" not in params:\n",
    "            params[\"ETag\"] = response.headers.get(\"ETag\").strip('\\\"')    \n",
    "        \n",
    "\n",
    "pd.json_normalize(contracts).to_parquet(\"./api_database/contracts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contracts = pd.read_parquet(\"./api_database/contracts.parquet\")[[\"contract_id\", \"date_issued\"]]\n",
    "\n",
    "known_abyssal_contracts = pd.read_parquet(\"./api_database/item_stats.parquet\")\n",
    "known_abyssal_contracts_agg = known_abyssal_contracts.groupby(\"contract_id\", as_index=False).agg({\n",
    "    \"item_id\": \"nunique\"\n",
    "}).rename(columns={\"item_id\":\"abyssal_count\"})\n",
    "\n",
    "df_contracts = df_contracts.merge(known_abyssal_contracts_agg,how=\"left\", on =\"contract_id\")\n",
    "df_contracts[\"date_issued\"] = pd.to_datetime(df_contracts[\"date_issued\"])\n",
    "df_contracts = df_contracts[(df_contracts[\"date_issued\"] > (pd.to_datetime(datetime.utcnow() - timedelta(days=1)).tz_localize(\"UTC\"))) | (df_contracts[\"abyssal_count\"] > 0)][[\"contract_id\"]]\n",
    "\n",
    "last_run_items = pd.read_parquet(\"./api_database/contract_items.parquet\")\n",
    "last_run_contracts = last_run_items.groupby(\"contract_id\", as_index=False).agg({\n",
    "    \"updated_at\": \"max\"\n",
    "})\n",
    "\n",
    "contracts_to_scan = df_contracts.merge(last_run_contracts, how=\"left\", on=\"contract_id\")\n",
    "contracts_to_scan = contracts_to_scan[(contracts_to_scan[\"updated_at\"] < (pd.to_datetime(\"today\") + pd.DateOffset(hours=-12))) | (contracts_to_scan[\"updated_at\"].isna())] [\"contract_id\"].drop_duplicates(keep=\"first\").tolist()\n",
    "\n",
    "last_run_items = last_run_items.merge(df_contracts[[\"contract_id\"]].drop_duplicates(keep=\"first\"), how=\"inner\", on=\"contract_id\")\n",
    "contracts_scanned = last_run_items[last_run_items[\"updated_at\"] > (pd.to_datetime(\"today\") + pd.DateOffset(hours=-12))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contracts_to_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "contract_count = 0\n",
    "print (len(contracts_to_scan))\n",
    "for contract_id in contracts_to_scan:\n",
    "    total_pages = 1\n",
    "    page = 1\n",
    "    while page <= total_pages:\n",
    "        contract_count += 1\n",
    "        base_url = \"https://esi.evetech.net/latest/contracts/public/items/{}/\".format(contract_id)\n",
    "        params = {\"datasource\": \"tranquility\", \"page\": page}\n",
    "                \n",
    "        print(base_url)\n",
    "        print(params)\n",
    "        print(contract_count)\n",
    "        response = requests.get(base_url, params=params)\n",
    "        print(response.headers)\n",
    "        print(response)\n",
    "        if (total_pages == 1) & (response.headers.get(\"X-Pages\") != None):\n",
    "            total_pages = int(response.headers.get(\"X-Pages\"))\n",
    "            print(total_pages)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            if response.text != \"\":\n",
    "                raw_contract_items = response.json()\n",
    "                contract_items = [contract_item for contract_item in raw_contract_items]\n",
    "                print(f\"Items found: {len(contract_items)}\")\n",
    "                for contract_item in contract_items:\n",
    "                    contract_item[\"contract_id\"] = contract_id\n",
    "                    contract_item[\"updated_at\"] = datetime.utcnow()\n",
    "                items.extend(contract_items)\n",
    "        else:\n",
    "            if int(response.headers.get(\"X-Esi-Error-Limit-Remain\", 100)) < 20:\n",
    "                break\n",
    "            elif int(response.headers.get(\"X-Esi-Error-Limit-Remain\", 50)) < 50:\n",
    "                time.sleep(60)\n",
    "        page += 1\n",
    "\n",
    "contracts_scanned = pd.concat([contracts_scanned, pd.json_normalize(items)],ignore_index=True)\n",
    "contracts_scanned.to_parquet(\"./api_database/contract_items.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_abyssal_contracts = pd.read_parquet(\"./api_database/item_stats.parquet\")\n",
    "known_abyssal_contracts_agg = known_abyssal_contracts.groupby([\"contract_id\"], as_index=False).agg({\n",
    "    \"item_id\": \"nunique\"\n",
    "}).rename(columns={\"item_id\":\"item_id_count\"})\n",
    "\n",
    "# Clean up affteragg data TODO\n",
    "contract_items = pd.read_parquet(\"./api_database/contract_items.parquet\")\n",
    "contract_items = contract_items.merge(known_abyssal_contracts_agg, how=\"left\", on=[\"contract_id\"])\n",
    "contract_items_to_scan = contract_items[(contract_items[\"item_id_count\"].isna()) & (contract_items[\"item_id\"].notna())]\n",
    "contract_items_to_scan[\"item_id\"] = contract_items_to_scan[\"item_id\"].astype(\"int64\")\n",
    "contract_items_to_scan[\"item_idtype_id\"] = contract_items_to_scan[\"type_id\"].astype(\"int64\")\n",
    "\n",
    "contract_items_scanned = contract_items[(contract_items[\"item_id_count\"].notna())]\n",
    "# contract_items_scanned = known_abyssal_contracts.merge(contract_items_scanned[[\"contract_id\"]].drop_duplicates(keep=\"first\"), how=\"inner\", on=\"contract_id\")\n",
    "contract_items_scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats = []\n",
    "\n",
    "for index, row in contract_items_to_scan.iterrows():\n",
    "    if row[\"type_id\"] in [49726, 49722, 49730]:\n",
    "        base_url = f\"https://esi.evetech.net/latest/dogma/dynamic/items/{row['type_id']}/{row['item_id']}/\"\n",
    "        params = {\"datasource\": \"tranquility\"}\n",
    "        print(base_url)\n",
    "        print(params)\n",
    "        response = requests.get(base_url, params=params)\n",
    "        print(response.headers)\n",
    "        print(response)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            row_info = response.json()\n",
    "            dogma_details = []\n",
    "            dogma_details = [mod for mod in row_info[\"dogma_attributes\"] if mod[\"attribute_id\"] in [50,64,204]]\n",
    "            for mod in dogma_details:\n",
    "                mod[\"contract_id\"] = row[\"contract_id\"]\n",
    "                mod[\"type_id\"] = row[\"type_id\"]\n",
    "                mod[\"item_id\"] = row[\"item_id\"]\n",
    "            item_stats.extend(dogma_details)\n",
    "        else:\n",
    "            if int(response.headers[\"X-Esi-Error-Limit-Remain\"]) < 20:\n",
    "                break\n",
    "            elif int(response.headers[\"X-Esi-Error-Limit-Remain\"]) < 50:\n",
    "                time.sleep(60)\n",
    "                \n",
    "df_pivoted = pd.json_normalize(item_stats).pivot(index=['contract_id', 'type_id', 'item_id'], columns='attribute_id', values='value').reset_index()\n",
    "df_pivoted = df_pivoted.rename_axis(None, axis=1)\n",
    "df_pivoted = df_pivoted.rename(columns={50: \"CPU\", 64: \"Damage\" , 204:\"ROF\"})\n",
    "output = pd.concat([contract_items_scanned, df_pivoted], ignore_index=True).drop_duplicates(keep=\"first\")\n",
    " \t\t\n",
    "output.to_parquet(\"./api_database/item_stats.parquet\")\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dict = {\"paladin\":\n",
    "        {\"damage\" :{\n",
    "            \"guns\": 3.6\n",
    "            ,\"ammo\": 70.8\n",
    "            ,\"hull\": 2\n",
    "            ,\"weapon\": 1.25\n",
    "            ,\"surgical_strike\": 1.15\n",
    "            ,\"spec\": 1.1\n",
    "            ,\"mar\" : 1.25\n",
    "            ,\"LE-1006\": 1.06\n",
    "            }\n",
    "        ,\"rof\": {\n",
    "            \"guns\": 7.875\n",
    "            ,\"gunnery\": 0.9\n",
    "            ,\"rapid_fire\": 0.8\n",
    "            ,\"RF-906\": 0.94\n",
    "            ,\"rig\": 0.85\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "for ship in ship_dict:\n",
    "    dps = 1.0\n",
    "    rof = 1.0\n",
    "    for key, value in ship_dict[ship][\"damage\"].items():\n",
    "        dps = dps * ship_dict[ship][\"damage\"][key]  \n",
    "    for key, value in ship_dict[ship][\"rof\"].items():\n",
    "        rof = rof * ship_dict[ship][\"rof\"][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import sys\n",
    "from more_itertools import chunked\n",
    "import os\n",
    "from reference import MODS, COLUMNS, SHIPS, start_logging, price_df_norm\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "start_logging('./info_log/matching_mods.log', __name__)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Script started at %s', datetime.datetime.now())\n",
    "\n",
    "Files = MODS\n",
    "\n",
    "# if len(sys.argv) > 1:\n",
    "#     if sys.argv[1] == \"HeatSink\":\n",
    "#         Files = [MODS[0]]\n",
    "#     elif sys.argv[1] == \"MagStab\":\n",
    "#         Files = [MODS[1]]\n",
    "#     elif sys.argv[1] == \"GyroStab\":\n",
    "#         Files = [MODS[2]]\n",
    "# else:\n",
    "#     Files = MODS\n",
    "\n",
    "chunkcount = 0\n",
    "\n",
    "\n",
    "def stacking_penalty(u):\n",
    "    result = math.exp(-(u/2.67)**2)\n",
    "    return result\n",
    "\n",
    "stacking_penalty_1 = stacking_penalty(1)\n",
    "stacking_penalty_2 = stacking_penalty(2)\n",
    "stacking_penalty_3 = stacking_penalty(3)\n",
    "stacking_penalty_4 = stacking_penalty(4)\n",
    "\n",
    "for Output in Files:\n",
    "    combination_data = pd.read_csv(f\".\\webcrawler\\{Output[0]}Output.csv\") \n",
    "    filepath = \"./sets/\"+ Output[0] +\"_sets.csv\"\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "        logger.info(\"Outdated file removed\")\n",
    "    logger.info(\"Creating new file\") \n",
    "\n",
    "    combination_data = price_df_norm(combination_data)\n",
    "    # combination_data[\"ROF\"] = ((combination_data[\"ROF\"] - 1) * 100).abs()\n",
    "    data = combination_data[(combination_data[\"DPS\"] >= 26) & (combination_data[\"Price\"] <= 1000000000)]\n",
    "\n",
    "    combination_data_list = list(data[\"ID\"])\n",
    "    combination_data_list = combination_data_list\n",
    "    logger.info(\"combination_data list created\")\n",
    "    combination_data_list2 = list(combinations(combination_data_list, (4)))\n",
    "        \n",
    "    size = 3000000\n",
    "    chunked_list = list(chunked(combination_data_list2, size))\n",
    "    logger.info(f\"chunked_list size: {size}\")\n",
    "    logger.info(f\"chunked_list len: {len(chunked_list)}\")\n",
    "    data = data.astype({\n",
    "        \"Damage\": float,\n",
    "        \"ROF\": float,\n",
    "        \"CPU\": float,\n",
    "        \"Contract\": str\n",
    "        })\n",
    "\n",
    "    data = data[[\"ID\", \"CPU\", 'Damage', 'ROF', \"DPS\", \"Contract\"]]\n",
    "\n",
    "    chunkcount = 0\n",
    "\n",
    "    ship_stats = SHIPS[Output[0]]\n",
    "  \n",
    "    for ship in ship_stats:\n",
    "        dps = 1.0\n",
    "        rof = 1.0\n",
    "        for key, value in ship_stats[\"damage\"].items():\n",
    "            dps = dps * ship_stats[\"damage\"][key]\n",
    "\n",
    "        for key, value in ship_stats[\"rof\"].items():\n",
    "            rof = rof * ship_stats[\"rof\"][key]\n",
    "        \n",
    "\n",
    "    \n",
    "    for chunk in chunked_list:\n",
    "        df = pd.DataFrame(chunk)\n",
    "\n",
    "        data2 = df.merge(data.add_suffix('_first'), left_on=0,right_on=\"ID_first\",how=\"left\")\n",
    "        data2 = data2.merge(data.add_suffix('_second'), left_on=1,right_on=\"ID_second\",how=\"left\")\n",
    "        data2 = data2.merge(data.add_suffix('_third'), left_on=2,right_on=\"ID_third\",how=\"left\")\n",
    "        data2 = data2.merge(data.add_suffix('_fourth'), left_on=3,right_on=\"ID_fourth\",how=\"left\")\n",
    "\n",
    "        data2damage = data2[[\"Damage_first\",\"Damage_second\",\"Damage_third\",\"Damage_fourth\"]]\n",
    "        data2damage = data2damage.rank(method=\"first\", axis=1, ascending=False)\n",
    "        data2damage = data2damage.replace({2:stacking_penalty_1, 3:stacking_penalty_2, 4:stacking_penalty_3})\n",
    "\n",
    "        data2rof = data2[[\"ROF_first\",\"ROF_second\",\"ROF_third\",\"ROF_fourth\"]]\n",
    "        data2rof = data2rof.rank(method=\"first\", axis=1, ascending=True)\n",
    "        data2rof = data2rof.replace({1:stacking_penalty_1, 2:stacking_penalty_2, 3:stacking_penalty_3, 4:stacking_penalty_4})\n",
    "\n",
    "        data3 = data2.merge(data2damage.add_suffix('_damagaPenalty'),how=\"left\",left_index=True, right_index=True)\n",
    "        data3 = data3.merge(data2rof.add_suffix('_rofPenalty'),how=\"left\",left_index=True, right_index=True)\n",
    "        data3[\"raw_damage\"] = dps\n",
    "        for col in [\"Damage_first\", \"Damage_second\", \"Damage_third\", \"Damage_fourth\"]:\n",
    "            data3[col] = ((data3[col] - 1) * data3[f\"{col}_damagaPenalty\"]) + 1 \n",
    "            data3[\"raw_damage\"] = data3[\"raw_damage\"] * data3[col]\n",
    "\n",
    "        data3[\"raw_rof\"] = rof\n",
    "        for col in [\"ROF_first\",\"ROF_second\",\"ROF_third\",\"ROF_fourth\"]:\n",
    "            data3[col] = 1 - ((1 - data3[col]) * data3[f\"{col}_rofPenalty\"])\n",
    "            data3[\"raw_rof\"] = data3[\"raw_rof\"] * data3[col]\n",
    "\n",
    "        data3[\"Total Damage\"] = ((data3[\"raw_damage\"] * 4) / (data3[\"raw_rof\"]/1000)) * 2\n",
    "        \n",
    "        data3[\"TotalCPU\"] = data3[\"CPU_first\"] + data3[\"CPU_second\"] + data3[\"CPU_third\"] + data3[\"CPU_fourth\"] \n",
    "        \n",
    "        if Output[1] == \"Market HeatSinks\":       \n",
    "            # Paladin\n",
    "            # report2 = data3[(data3[\"TotalDamage\"] >= 4.355) & (data3[\"TotalROF\"] >= 33.11)].copy()\n",
    "            report2 = data3[(data3[\"Total Damage\"] >= 3350)].copy()\n",
    "        elif Output[1] == \"Market GyroStabs\": \n",
    "            # Vargur \n",
    "            # report2 = data3[(data3[\"TotalDamage\"] >= 4.369) & (data3[\"TotalROF\"] >= 33.1114)].copy()\n",
    "            report2 = data3[(data3[\"Total Damage\"] >= 3350)].copy()\n",
    "        elif Output[1] == \"Market MagStabs\": \n",
    "            # Kronos\n",
    "            # report2 = data3[(data3[\"TotalDamage\"] >= 4.368) & (data3[\"TotalROF\"] >= 31.23)].copy()\n",
    "            report2 = data3[(data3[\"Total Damage\"] >= 4100)].copy()\n",
    "\n",
    "        report3 = report2[[0,1,2,3,\"Total Damage\"]]     \n",
    "  \n",
    "        report2.to_csv(filepath, mode='a', index=False, header=True)\n",
    "        chunkcount = chunkcount+1\n",
    "        logger.info(f\"{Output[0]} chunk \" + str(chunkcount) + \" saved.\")    \n",
    "        \n",
    "    logger.info(f\"All {Output[0]} chunks saved.\")\n",
    "    \n",
    "logger.info('Script completed at %s', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =pd.read_csv(\"D:\\GitHub\\Incursion-Damage-Mods\\sets\\HeatSink_sets.csv\")\n",
    "df[[\"0\",\"1\",\"2\",\"3\",\"TOTAL DAMAGE\"]].sort_values(by=\"TOTAL DAMAGE\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"D:\\GitHub\\Incursion-Damage-Mods\\sets\\MagStab_sets.csv\")[[\"0\",\"1\",\"2\",\"3\",\"TOTAL DAMAGE\"]].sort_values(by=\"TOTAL DAMAGE\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:\\GitHub\\Incursion-Damage-Mods\\sets\\GyroStab_sets.csv\",low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3[(data3[0] == \"https://mutaplasmid.space/module/1040887384497/\") & (data3[1] == \"https://mutaplasmid.space/module/1037739435504/\") & (data3[2] == \"https://mutaplasmid.space/module/1043429418844/\")& (data3[3] == \"https://mutaplasmid.space/module/1043508303551/\")][[\"ROF_first\",\"ROF_second\",\"ROF_third\",\"ROF_fourth\", 'ROF_first_rofPenalty',       'ROF_second_rofPenalty',\n",
    "              'ROF_third_rofPenalty',       'ROF_fourth_rofPenalty',]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
